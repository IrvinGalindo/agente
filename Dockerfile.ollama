# Dockerfile.ollama - Ollama con modelo TinyLlama pre-instalado
FROM ollama/ollama:latest

# Metadatos
LABEL description="Ollama con TinyLlama pre-instalado para Render gratuito"
LABEL version="1.0.0"

# Variables de entorno
ENV OLLAMA_HOST=0.0.0.0
ENV OLLAMA_MODELS=/root/.ollama/models
ENV OLLAMA_KEEP_ALIVE=5m
ENV OLLAMA_MAX_LOADED_MODELS=1

# Crear directorio para modelos
RUN mkdir -p /root/.ollama/models

# Script para instalar modelo en segundo plano
COPY <<EOF /install-models.sh
#!/bin/bash
set -e

echo "ü¶ô Iniciando Ollama server..."
ollama serve &
OLLAMA_PID=\$!

# Esperar a que Ollama est√© listo
echo "‚è≥ Esperando que Ollama est√© listo..."
for i in {1..30}; do
    if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
        echo "‚úÖ Ollama est√° listo"
        break
    fi
    echo "Intento \$i/30..."
    sleep 2
done

# Verificar si el modelo ya existe
if curl -s http://localhost:11434/api/tags | grep -q "tinyllama"; then
    echo "‚úÖ TinyLlama ya est√° instalado"
else
    echo "üì• Descargando TinyLlama (esto puede tomar varios minutos)..."
    
    # Instalar TinyLlama (modelo m√°s peque√±o y eficiente)
    ollama pull tinyllama:1.1b-chat-q4_0 || {
        echo "‚ùå Error instalando tinyllama:1.1b-chat-q4_0, intentando versi√≥n b√°sica..."
        ollama pull tinyllama || {
            echo "‚ö†Ô∏è No se pudo instalar TinyLlama, continuando sin modelo pre-instalado"
        }
    }
fi

# Verificar modelos instalados
echo "üìã Modelos disponibles:"
ollama list

echo "‚úÖ Configuraci√≥n completa"

# Mantener Ollama corriendo
wait \$OLLAMA_PID
EOF

# Hacer ejecutable el script
RUN chmod +x /install-models.sh

# Script de inicio que instala modelos si no existen
COPY <<EOF /start.sh
#!/bin/bash
set -e

echo "üöÄ Iniciando Ollama con modelos pre-configurados..."

# Verificar si ya hay modelos instalados
if [ -z "\$(ls -A /root/.ollama/models 2>/dev/null)" ]; then
    echo "üì¶ Primera ejecuci√≥n - instalando modelos..."
    /install-models.sh
else
    echo "‚úÖ Modelos ya instalados, iniciando servidor..."
    exec ollama serve
fi
EOF

RUN chmod +x /start.sh

# Healthcheck
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:11434/api/tags || exit 1

# Exponer puerto
EXPOSE 11434

# Comando de inicio
CMD ["/start.sh"]