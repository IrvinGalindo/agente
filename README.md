ğŸ³ Multi-App Docker Stack con Ollama Optimizado
Este proyecto contiene una configuraciÃ³n Docker Compose con mÃºltiples aplicaciones integradas, optimizada para Render gratuito con modelos LLM pre-instalados.

ğŸ› ï¸ Servicios Incluidos
Servicio	Puerto	DescripciÃ³n	URL
ğŸ¦™ Ollama Manager	3000	Interface web para gestionar Ollama	/dashboard/
ğŸ“± Evolution API	8080	API completa para WhatsApp Business	/evolution/
âš¡ n8n	5678	AutomatizaciÃ³n visual de workflows	/n8n/
ğŸ¤– Ollama	11434	Servidor LLM con TinyLlama pre-instalado	/ollama/
ğŸ—„ï¸ PostgreSQL	5432	Base de datos principal	-
âš¡ Redis	6379	Cache y almacenamiento rÃ¡pido	-
ğŸŒ Nginx	80	Proxy reverso (puerto principal)	/
ğŸ“ Estructura del Proyecto
mi-proyecto-docker/
â”œâ”€â”€ docker-compose.yml          # ConfiguraciÃ³n principal
â”œâ”€â”€ .env                       # Variables de entorno
â”œâ”€â”€ Dockerfile.ollama          # Ollama con modelo pre-instalado
â”œâ”€â”€ Dockerfile.manager         # Interface web para Ollama
â”œâ”€â”€ Dockerfile.nginx           # Proxy reverso
â”œâ”€â”€ start.sh                   # Script de inicio automÃ¡tico
â”œâ”€â”€ .dockerignore             # Archivos a excluir del build
â”œâ”€â”€ README.md                  # Esta documentaciÃ³n
â””â”€â”€ render.yaml               # ConfiguraciÃ³n para Render
ğŸš€ InstalaciÃ³n RÃ¡pida
1. Descargar archivos
bash
mkdir mi-multiapp-docker
cd mi-multiapp-docker
# Copia todos los archivos aquÃ­
2. Configurar variables (IMPORTANTE)
bash
# Editar .env y cambiar TODAS las contraseÃ±as:
nano .env

# Cambiar obligatoriamente:
POSTGRES_PASSWORD=PostgreSQL_Seguro_2024!
REDIS_PASSWORD=Redis_Seguro_2024!
EVOLUTION_API_KEY=Evolution_API_Key_Muy_Segura_2024!
N8N_BASIC_AUTH_PASSWORD=N8N_Password_Seguro_2024!
N8N_ENCRYPTION_KEY=N8N_Encryption_Key_Super_Larga_Y_Muy_Segura_2024_123456789
3. Ejecutar localmente
bash
# Dar permisos al script
chmod +x start.sh

# Iniciar todo automÃ¡ticamente
./start.sh

# O manualmente:
docker-compose up -d
4. Acceder a los servicios
Dashboard Principal: http://localhost/dashboard/
Evolution API: http://localhost/evolution/
n8n Workflows: http://localhost/n8n/
Ollama API: http://localhost/ollama/
Info General: http://localhost/info
ğŸ¦™ Ollama Optimizado para Render Gratuito
Modelos Pre-instalados:
âœ… TinyLlama (637MB) - Instalado automÃ¡ticamente
âœ… TinyLlama Chat (400MB) - VersiÃ³n optimizada para chat
âš ï¸ Qwen2 0.5B (500MB) - Disponible para instalaciÃ³n manual
GestiÃ³n desde el Dashboard:
Ve a /dashboard/
Usa la secciÃ³n "ğŸ“¦ Modelos Disponibles"
Haz clic en "TinyLlama" para instalarlo
Prueba el modelo en la secciÃ³n "ğŸ’¬ Probar Modelo"
API Direct:
bash
# Listar modelos
curl http://localhost/ollama/api/tags

# Generar texto
curl -X POST http://localhost/ollama/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "tinyllama",
    "prompt": "Hola, Â¿cÃ³mo estÃ¡s?",
    "stream": false
  }'
ğŸŒ Despliegue en Render
MÃ©todo 1: Docker Compose
Sube el cÃ³digo a GitHub
Conecta tu repo a Render
Crear Web Service:
Build Command: docker-compose build
Start Command: docker-compose up nginx
Port: 80
MÃ©todo 2: Variables de Entorno en Render
En Render Dashboard > Environment, agregar:

bash
POSTGRES_PASSWORD=PostgreSQL_Seguro_2024!
REDIS_PASSWORD=Redis_Seguro_2024!
EVOLUTION_API_KEY=Evolution_API_Key_Muy_Segura_2024!
N8N_BASIC_AUTH_USER=admin
N8N_BASIC_AUTH_PASSWORD=N8N_Password_Seguro_2024!
N8N_ENCRYPTION_KEY=N8N_Encryption_Key_Super_Larga_Y_Muy_Segura_2024_123456789
N8N_WEBHOOK_URL=https://tu-app.onrender.com
PORT=80
URLs en Render:
Una vez desplegado:

Dashboard: https://tu-app.onrender.com/dashboard/
Evolution: https://tu-app.onrender.com/evolution/
n8n: https://tu-app.onrender.com/n8n/
Ollama: https://tu-app.onrender.com/ollama/
ğŸ” Credenciales por Defecto
n8n
Usuario: admin
ContraseÃ±a: N8N_Password_Seguro_2024!
Evolution API
API Key: Evolution_API_Key_Muy_Segura_2024!
Base de datos PostgreSQL
Usuario: admin
ContraseÃ±a: PostgreSQL_Seguro_2024!
Base: multiapp_db
ğŸ“Š Uso de las APIs
Evolution API - WhatsApp
bash
# Crear instancia
curl -X POST https://tu-app.onrender.com/evolution/instance/create \
  -H "apikey: Evolution_API_Key_Muy_Segura_2024!" \
  -H "Content-Type: application/json" \
  -d '{"instanceName": "mi-whatsapp"}'

# Obtener QR
curl https://tu-app.onrender.com/evolution/instance/qrcode/mi-whatsapp \
  -H "apikey: Evolution_API_Key_Muy_Segura_2024!"
n8n - Workflows
Ve a /n8n/
Login: admin / N8N_Password_Seguro_2024!
Crea workflows arrastrando nodos
Conecta con Evolution API y Ollama
Ollama - LLM
bash
# Modelos disponibles
curl https://tu-app.onrender.com/ollama/api/tags

# Generar texto
curl -X POST https://tu-app.onrender.com/ollama/api/generate \
  -d '{"model":"tinyllama","prompt":"Explica quÃ© es Docker","stream":false}'

# Instalar nuevo modelo (solo pequeÃ±os)
curl -X POST https://tu-app.onrender.com/ollama/api/pull \
  -d '{"name":"qwen2:0.5b"}'
ğŸ”§ Comandos Ãštiles
Desarrollo Local
bash
# Ver logs en vivo
docker-compose logs -f

# Restart solo un servicio
docker-compose restart ollama

# Rebuild sin cache
docker-compose build --no-cache ollama

# Ver estado de servicios
docker-compose ps

# Entrar a un contenedor
docker-compose exec ollama bash
Monitoreo
bash
# Uso de recursos
docker stats

# Logs especÃ­ficos
docker-compose logs ollama
docker-compose logs evolution-api

# Limpiar todo
docker-compose down -v
Backup
bash
# Backup PostgreSQL
docker-compose exec postgres pg_dump -U admin multiapp_db > backup.sql

# Backup volÃºmenes
docker run --rm -v mi-proyecto-docker_postgres_data:/data \
  -v $(pwd):/backup alpine \
  tar czf /backup/postgres_backup.tar.gz /data
âš ï¸ Limitaciones Render Gratuito
Recursos:
RAM: 512MB (suficiente para TinyLlama)
CPU: Limitada
Disco: Temporal (volÃºmenes se pierden al dormir)
Sleep: 15 min sin actividad
Soluciones:
Modelos pequeÃ±os: Solo TinyLlama, Qwen2 0.5B
Keep-alive: Ping automÃ¡tico cada 10 min
Re-instalaciÃ³n: Scripts automÃ¡ticos al despertar
Upgrade: Plan Pro ($7/mes) = 2GB RAM
ğŸ” Troubleshooting
Problemas Comunes
âŒ Ollama no instala modelos

bash
# Ver logs
docker-compose logs ollama

# Restart manual
docker-compose restart ollama

# Instalar desde dashboard
# Ve a /dashboard/ > "TinyLlama"
âŒ n8n no conecta a PostgreSQL

bash
# Verificar PostgreSQL
docker-compose exec postgres pg_isready -U admin

# Verificar variables
docker-compose config
âŒ Evolution API no inicia

bash
# Ver logs especÃ­ficos
docker-compose logs evolution-api

# Verificar Redis
docker-compose exec redis redis-cli ping
Logs de Debug
bash
# Todos los logs
docker-compose logs

# Solo errores
docker-compose logs | grep ERROR

# Ãšltimas 50 lÃ­neas
docker-compose logs --tail=50
ğŸš€ Optimizaciones Avanzadas
Para Render Pro (2GB RAM)
bash
# En .env cambiar:
OLLAMA_MAX_LOADED_MODELS=2
# Y agregar modelos mÃ¡s grandes:
# phi3:mini, llama3.2:1b
Keep-Alive AutomÃ¡tico
bash
# Crear cron job (en tu servidor local)
*/10 * * * * curl -s https://tu-app.onrender.com/health > /dev/null
Auto-rebuild en GitHub Actions
yaml
# .github/workflows/deploy.yml
name: Deploy to Render
on:
  push:
    branches: [main]
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Trigger Render Deploy
        run: |
          curl -X POST "https://api.render.com/deploy/srv-YOUR_SERVICE_ID"
ğŸ¯ Casos de Uso
1. Bot de WhatsApp con IA
Evolution API recibe mensajes
n8n procesa y filtra
Ollama genera respuestas inteligentes
Evolution API envÃ­a respuesta
2. AutomatizaciÃ³n de Procesos
n8n monitorea APIs/emails
Procesa con Ollama
EnvÃ­a notificaciones por WhatsApp
Guarda datos en PostgreSQL
3. Dashboard de IA
Interface web para mÃºltiples LLMs
Chat directo con modelos
GestiÃ³n de conversaciones
Analytics y reportes
ğŸ“ Soporte
GitHub Issues: Para reportar bugs
Render Docs: https://render.com/docs
Docker Docs: https://docs.docker.com
Ollama Models: https://ollama.ai/library
**ğŸ‰ Â¡Tu stack multi-app estÃ¡ listo para producciÃ³n!**bash

Cambiar TODAS las contraseÃ±as por unas seguras
POSTGRES_PASSWORD=tu_password_postgresql_seguro_123
REDIS_PASSWORD=tu_password_redis_seguro_123
EVOLUTION_API_KEY=tu_api_key_evolution_muy_segura_123
N8N_BASIC_AUTH_PASSWORD=tu_password_n8n_seguro_123
N8N_ENCRYPTION_KEY=tu_clave_encriptacion_n8n_muy_larga_y_segura_123456789

Para producciÃ³n, cambiar la URL de webhook
N8N_WEBHOOK_URL=https://tu-dominio.onrender.com


### 3. Ejecutar Localmente

```bash
# Iniciar todos los servicios
docker-compose up -d

# Ver logs
docker-compose logs -f

# Parar servicios
docker-compose down
ğŸŒ Despliegue en Render
OpciÃ³n 1: Docker Compose (Recomendado)
Crear nuevo Web Service en Render
Conectar tu repositorio de GitHub
Configurar el servicio:
Build Command: docker-compose build
Start Command: docker-compose up
Port: 8080
Agregar Variables de Entorno: En Render Dashboard > Environment, agregar todas las variables del archivo .env
OpciÃ³n 2: Servicios Separados
Crear un servicio separado para cada aplicaciÃ³n:

Evolution API: Puerto 8080
n8n: Puerto 5678
Ollama: Puerto 11434
PostgreSQL: Como base de datos externa
Redis: Como servicio de cache
ğŸ“‹ URLs de Acceso
Una vez desplegado, podrÃ¡s acceder a:

Evolution API: https://tu-app.onrender.com:8080
n8n Interface: https://tu-app.onrender.com:5678
Ollama API: https://tu-app.onrender.com:11434
ğŸ” Credenciales por Defecto
n8n
Usuario: admin (configurable en .env)
ContraseÃ±a: Definida en N8N_BASIC_AUTH_PASSWORD
Evolution API
API Key: Definida en EVOLUTION_API_KEY
ğŸ“Š Uso de las APIs
Evolution API - Ejemplos
bash
# Crear instancia de WhatsApp
curl -X POST https://tu-app.onrender.com:8080/instance/create \
  -H "apikey: tu_api_key_evolution_muy_segura_123" \
  -H "Content-Type: application/json" \
  -d '{"instanceName": "mi-instancia"}'

# Obtener QR Code
curl https://tu-app.onrender.com:8080/instance/qrcode/mi-instancia \
  -H "apikey: tu_api_key_evolution_muy_segura_123"
Ollama - Ejemplos
bash
# Descargar modelo
curl -X POST https://tu-app.onrender.com:11434/api/pull \
  -d '{"name": "llama2"}'

# Generar texto
curl -X POST https://tu-app.onrender.com:11434/api/generate \
  -d '{"model": "llama2", "prompt": "Hola, Â¿cÃ³mo estÃ¡s?"}'
ğŸ”§ ConfiguraciÃ³n Avanzada
Modelos de Ollama
Para instalar modelos especÃ­ficos, conÃ©ctate al contenedor:

bash
docker exec -it ollama ollama pull llama2
docker exec -it ollama ollama pull mistral
docker exec -it ollama ollama pull codellama
Backups
bash
# Backup de PostgreSQL
docker exec postgres pg_dump -U admin multiapp_db > backup.sql

# Backup de volÃºmenes
docker run --rm -v mi-proyecto-docker_postgres_data:/data -v $(pwd):/backup alpine tar czf /backup/postgres_backup.tar.gz /data
ğŸ› Troubleshooting
Problemas Comunes
Puertos en uso: Cambiar puertos en docker-compose.yml
Memoria insuficiente: Reducir servicios o aumentar recursos
Permisos: Verificar que Docker tenga permisos necesarios
Logs Ãštiles
bash
# Ver logs de todos los servicios
docker-compose logs

# Logs de un servicio especÃ­fico
docker-compose logs evolution-api
docker-compose logs n8n
docker-compose logs ollama
ğŸ“„ Licencias
Evolution API: Consultar repositorio oficial
n8n: Fair-code distributed license
Ollama: MIT License
PostgreSQL: PostgreSQL License
Redis: BSD License
ğŸ¤ Contribuir
Fork el proyecto
Crear feature branch (git checkout -b feature/AmazingFeature)
Commit cambios (git commit -m 'Add some AmazingFeature')
Push a la branch (git push origin feature/AmazingFeature)
Abrir Pull Request
ğŸ“ Soporte
Para soporte tÃ©cnico:

Abrir issue en GitHub
Consultar documentaciÃ³n oficial de cada servicio
Verificar logs de contenedores
Â¡Listo para usar! ğŸš€

